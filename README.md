# Data and script used for the paper "An Empirical Study on the Stability of Explainable Software Defect Prediction"


## 1. Dataset
- The dataset used for the paper is provided in the ''dataset'' folder.
- It contains total of 32 versions comprised with 9 different projects.


## 2. Scripts in Jupyter Notebook
- We also provide the scripts we used to investigate our research questions.

### RQ1: Are the generated explanations from the same technique consistent under different data sampling techniques?
- In this RQ, we apply different data sampling techniques and compare whether they have different explanations generated or not.
- To run the scripts, open the Jupyter Notebook on one of the sampling techniques.
- Then hit the 'Run All' button to execute the script.


### RQ2: Are the generated explanations from the same technique consistent under different ML classifiers?
- In this RQ, we apply different machine learning classifiers and compare whether they have different explanations generated or not.
- To run the scripts, open the Jupyter Notebook on one of the sampling techniques.
- Then hit the 'Run All' button to execute the script.

### RQ3: Are the generated explanations from the same technique consistent under cross-version SDP scenarios?
- In this RQ, we apply different SDP scenario, i.e. cross-version defect prediction, and compare whether they have different explanations generated or not.
- To run the scripts, open the Jupyter Notebook on one of the sampling techniques.
- Then hit the 'Run All' button to execute the script.

### RQ4: Are the generated explanations from the same technique consistent under cross-project SDP scenarios?
- This RQ is not in the paper due to space constraint. However, we added to make the study more complete.
- In this RQ, we apply different SDP scenario, i.e. cross-project defect prediction, and compare whether they have different explanations generated or not.
- To run the scripts, open the Jupyter Notebook on one of the sampling techniques.
- Then hit the 'Run All' button to execute the script.

### Disucssion (JIT-DP)
- We also add the scripts we used in the Discussion section A. Stability of Explanations for Just-in-Time Defect Prediction. 
- Since our paper mainly focuses on file-level prediction models, this was set as a post-hoc experiment.
- To run the scripts, open the Jupyter Notebook on one of the sampling techniques.
- Then hit the 'Run All' button to execute the script.

